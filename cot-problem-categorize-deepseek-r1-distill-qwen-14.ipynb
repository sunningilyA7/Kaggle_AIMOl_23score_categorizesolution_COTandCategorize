{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb55560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T02:51:38.263408Z",
     "iopub.status.busy": "2024-10-25T02:51:38.262446Z",
     "iopub.status.idle": "2024-10-25T02:51:38.269929Z",
     "shell.execute_reply": "2024-10-25T02:51:38.26859Z",
     "shell.execute_reply.started": "2024-10-25T02:51:38.263362Z"
    },
    "papermill": {
     "duration": 0.004812,
     "end_time": "2025-02-18T01:45:38.420581",
     "exception": false,
     "start_time": "2025-02-18T01:45:38.415769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "References\n",
    "\n",
    "- https://www.kaggle.com/code/mpware/vllm-0-7 for the current installation script\n",
    "- https://www.kaggle.com/code/richolson/ai-math-olympiad-qwen2-5-72b for showing how to submit\n",
    "- https://www.kaggle.com/code/abdullahmeda/load-72b-awq-model-using-vllm-on-l4-x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accae258",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:45:38.430054Z",
     "iopub.status.busy": "2025-02-18T01:45:38.429845Z",
     "iopub.status.idle": "2025-02-18T01:45:38.433962Z",
     "shell.execute_reply": "2025-02-18T01:45:38.433358Z"
    },
    "papermill": {
     "duration": 0.009869,
     "end_time": "2025-02-18T01:45:38.434999",
     "exception": false,
     "start_time": "2025-02-18T01:45:38.425130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/560682#3113134\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6d4043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:45:38.444109Z",
     "iopub.status.busy": "2025-02-18T01:45:38.443906Z",
     "iopub.status.idle": "2025-02-18T01:45:52.569776Z",
     "shell.execute_reply": "2025-02-18T01:45:52.569027Z"
    },
    "papermill": {
     "duration": 14.131803,
     "end_time": "2025-02-18T01:45:52.571098",
     "exception": false,
     "start_time": "2025-02-18T01:45:38.439295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "start_time = time.time()\n",
    "cutoff_time = start_time + (4 * 60 + 45) * 60\n",
    "cutoff_times = [int(x) for x in np.linspace(cutoff_time, start_time + 60 * 60, 50 + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac7f9295",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-02-18T01:45:52.580858Z",
     "iopub.status.busy": "2025-02-18T01:45:52.580504Z",
     "iopub.status.idle": "2025-02-18T01:49:07.765395Z",
     "shell.execute_reply": "2025-02-18T01:49:07.764536Z"
    },
    "papermill": {
     "duration": 195.191235,
     "end_time": "2025-02-18T01:49:07.766955",
     "exception": false,
     "start_time": "2025-02-18T01:45:52.575720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-18 01:46:32 __init__.py:183] Automatically detected platform cuda.\n",
      "INFO 02-18 01:47:06 config.py:526] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 02-18 01:47:09 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 02-18 01:47:09 config.py:1383] Defaulting to use mp for distributed inference\n",
      "WARNING 02-18 01:47:09 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 02-18 01:47:09 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='/kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1', speculative_config=None, tokenizer='/kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=12288, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=2024, served_model_name=/kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[16,8,4,2,1],\"max_capture_size\":16}, use_cached_outputs=False, \n",
      "WARNING 02-18 01:47:10 multiproc_worker_utils.py:298] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-18 01:47:10 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 02-18 01:47:10 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 02-18 01:47:10 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 02-18 01:47:10 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "WARNING 02-18 01:47:11 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m WARNING 02-18 01:47:11 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 02-18 01:47:11 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 02-18 01:47:11 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 02-18 01:47:11 cuda.py:235] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 02-18 01:47:11 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 02-18 01:47:11 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 02-18 01:47:11 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 02-18 01:47:22 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 02-18 01:47:22 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 02-18 01:47:22 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 02-18 01:47:22 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 02-18 01:47:22 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "INFO 02-18 01:47:22 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 02-18 01:47:22 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 02-18 01:47:22 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "WARNING 02-18 01:47:23 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m WARNING 02-18 01:47:23 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 02-18 01:47:23 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 02-18 01:47:23 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 02-18 01:47:23 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_340eaa6e'), local_subscribe_port=57607, remote_subscribe_port=None)\n",
      "INFO 02-18 01:47:23 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 02-18 01:47:23 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1...\n",
      "INFO 02-18 01:47:23 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1...\n",
      "INFO 02-18 01:47:23 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f35a254b8404d5daceca2b7cb3b9050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 02-18 01:48:33 model_runner.py:1116] Loading model weights took 2.3731 GB\n",
      "INFO 02-18 01:48:33 model_runner.py:1116] Loading model weights took 2.3731 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 02-18 01:48:33 model_runner.py:1116] Loading model weights took 2.3731 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 02-18 01:48:33 model_runner.py:1116] Loading model weights took 2.3731 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 02-18 01:48:56 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 02-18 01:48:56 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 02-18 01:48:56 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m WARNING 02-18 01:48:56 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 02-18 01:48:56 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 02-18 01:48:56 worker.py:266] Memory profiling takes 22.07 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 02-18 01:48:56 worker.py:266] Memory profiling takes 22.06 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 02-18 01:48:56 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 02-18 01:48:56 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 02-18 01:48:56 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 02-18 01:48:56 worker.py:266] model weights take 2.37GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.82GiB; the rest of the memory reserved for KV Cache is 17.80GiB.\n",
      "INFO 02-18 01:48:56 worker.py:266] Memory profiling takes 22.07 seconds\r\n",
      "INFO 02-18 01:48:56 worker.py:266] model weights take 2.37GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.82GiB; the rest of the memory reserved for KV Cache is 17.80GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 02-18 01:48:56 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 02-18 01:48:56 worker.py:266] model weights take 2.37GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.82GiB; the rest of the memory reserved for KV Cache is 17.80GiB.\n",
      "WARNING 02-18 01:48:56 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 02-18 01:48:56 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 02-18 01:48:56 worker.py:266] Memory profiling takes 22.44 seconds\r\n",
      "INFO 02-18 01:48:56 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 02-18 01:48:56 worker.py:266] model weights take 2.37GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.82GiB; the rest of the memory reserved for KV Cache is 17.80GiB.\n",
      "INFO 02-18 01:48:57 executor_base.py:108] # CUDA blocks: 24301, # CPU blocks: 5461\n",
      "INFO 02-18 01:48:57 executor_base.py:113] Maximum concurrency for 12288 tokens per request: 31.64x\n",
      "WARNING 02-18 01:48:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 02-18 01:48:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 02-18 01:48:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m WARNING 02-18 01:48:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 02-18 01:48:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 02-18 01:48:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m WARNING 02-18 01:48:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 02-18 01:48:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 02-18 01:49:01 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 02-18 01:49:01 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-18 01:49:01 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-18 01:49:01 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  80%|████████  | 4/5 [00:04<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 02-18 01:49:07 model_runner.py:1563] Graph capturing finished in 6 secs, took 0.09 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 5/5 [00:05<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 02-18 01:49:07 model_runner.py:1563] Graph capturing finished in 6 secs, took 0.09 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 02-18 01:49:07 model_runner.py:1563] Graph capturing finished in 6 secs, took 0.09 GiB\n",
      "INFO 02-18 01:49:07 model_runner.py:1563] Graph capturing finished in 6 secs, took 0.09 GiB\n",
      "INFO 02-18 01:49:07 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 33.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if os.getenv('KAGGLE_KERNEL_RUN_TYPE') or os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    llm_model_pth = '/kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1'\n",
    "    # llm_model_pth = '/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1'\n",
    "else:\n",
    "    llm_model_pth = '/root/volume/KirillR/QwQ-32B-Preview-AWQ'\n",
    "\n",
    "MAX_NUM_SEQS = 14\n",
    "MAX_MODEL_LEN = 8192 * 3 // 2\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "    # dtype=\"half\",                # The data type for the model weights and activations\n",
    "    max_num_seqs=MAX_NUM_SEQS,   # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=MAX_MODEL_LEN, # Model context length\n",
    "    trust_remote_code=True,      # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=4,      # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.95, # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    "    seed=2024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a9cc07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:49:07.790410Z",
     "iopub.status.busy": "2025-02-18T01:49:07.790136Z",
     "iopub.status.idle": "2025-02-18T01:49:07.793724Z",
     "shell.execute_reply": "2025-02-18T01:49:07.793083Z"
    },
    "papermill": {
     "duration": 0.016222,
     "end_time": "2025-02-18T01:49:07.794767",
     "exception": false,
     "start_time": "2025-02-18T01:49:07.778545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.1\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "print(vllm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1caae81c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:49:07.817268Z",
     "iopub.status.busy": "2025-02-18T01:49:07.817047Z",
     "iopub.status.idle": "2025-02-18T01:49:07.819923Z",
     "shell.execute_reply": "2025-02-18T01:49:07.819305Z"
    },
    "papermill": {
     "duration": 0.015241,
     "end_time": "2025-02-18T01:49:07.820979",
     "exception": false,
     "start_time": "2025-02-18T01:49:07.805738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecda14e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:49:07.843431Z",
     "iopub.status.busy": "2025-02-18T01:49:07.843212Z",
     "iopub.status.idle": "2025-02-18T01:49:07.848288Z",
     "shell.execute_reply": "2025-02-18T01:49:07.847687Z"
    },
    "papermill": {
     "duration": 0.01739,
     "end_time": "2025-02-18T01:49:07.849322",
     "exception": false,
     "start_time": "2025-02-18T01:49:07.831932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import keyword\n",
    "\n",
    "\n",
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "def select_answer(answers):\n",
    "    counter = Counter()\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                counter[int(answer)] += 1 + random.random() / 1_000\n",
    "        except:\n",
    "            pass\n",
    "    if not counter:\n",
    "        return 210\n",
    "    _, answer = sorted([(v,k) for k,v in counter.items()], reverse=True)[0]\n",
    "    return answer%1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa7500ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:49:07.871656Z",
     "iopub.status.busy": "2025-02-18T01:49:07.871403Z",
     "iopub.status.idle": "2025-02-18T01:49:07.877275Z",
     "shell.execute_reply": "2025-02-18T01:49:07.876687Z"
    },
    "papermill": {
     "duration": 0.018188,
     "end_time": "2025-02-18T01:49:07.878283",
     "exception": false,
     "start_time": "2025-02-18T01:49:07.860095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "    max_tokens = MAX_MODEL_LEN\n",
    "    if time.time() > cutoff_times[-1]:\n",
    "        print(\"Speedrun\")\n",
    "        max_tokens = 2 * MAX_MODEL_LEN // 3\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,              # randomness of the sampling\n",
    "        min_p=0.01,\n",
    "        skip_special_tokens=True,     # Whether to skip special tokens in the output\n",
    "        max_tokens=max_tokens,\n",
    "        stop=[\"</think>\"]\n",
    "    )\n",
    "    \n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "\n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    \n",
    "    print([len(single_request_output.outputs[0].token_ids) for single_request_output in request_output])\n",
    "\n",
    "    sort_keys_and_list_of_messages = []\n",
    "\n",
    "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
    "        # print()\n",
    "        # print(single_request_output.outputs[0].text)\n",
    "        # print()\n",
    "        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n",
    "\n",
    "        sort_keys_and_list_of_messages.append(\n",
    "            (\n",
    "                len(single_request_output.outputs[0].token_ids),\n",
    "                messages\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print([sort_key for sort_key, _ in sort_keys_and_list_of_messages])\n",
    "    sort_keys_and_list_of_messages.sort(key=lambda sort_key_and_messages: sort_key_and_messages[0])\n",
    "    print([sort_key for sort_key, _ in sort_keys_and_list_of_messages])\n",
    "\n",
    "    list_of_messages = [messages for _, messages in sort_keys_and_list_of_messages]\n",
    "    \n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9afee724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:49:07.901002Z",
     "iopub.status.busy": "2025-02-18T01:49:07.900782Z",
     "iopub.status.idle": "2025-02-18T01:49:07.904263Z",
     "shell.execute_reply": "2025-02-18T01:49:07.903682Z"
    },
    "papermill": {
     "duration": 0.01577,
     "end_time": "2025-02-18T01:49:07.905249",
     "exception": false,
     "start_time": "2025-02-18T01:49:07.889479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_filter(list_of_messages) -> tuple[list[list[dict]], list[str]]:\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    for messages in list_of_messages:\n",
    "        answer = extract_boxed_text(messages[-1]['content'])\n",
    "        if answer:\n",
    "            extracted_answers.append(answer)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "    return list_of_messages_to_keep, extracted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f24473a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:49:07.928054Z",
     "iopub.status.busy": "2025-02-18T01:49:07.927825Z",
     "iopub.status.idle": "2025-02-18T01:49:07.933320Z",
     "shell.execute_reply": "2025-02-18T01:49:07.932691Z"
    },
    "papermill": {
     "duration": 0.018012,
     "end_time": "2025-02-18T01:49:07.934264",
     "exception": false,
     "start_time": "2025-02-18T01:49:07.916252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "thought_prefix_english = \"\"\"<think>Math Experts categorize question into one of the following types: \"Number Theory, Combinatorial Mathematics, Algebraic Inequalities, Geometric Constructions, Functional Equations, or Others.\"\n",
    "\"\"\"\n",
    "\n",
    "problem_cate = \"\"\"\n",
    "If the question is about Number Theory, please consider:\n",
    "Understand the problem and try simple examples; use basic tools (prime factorization, divisibility, modular arithmetic, Euclid’s algorithm); look for invariants or recursive patterns; simplify or transform conditions via induction.\n",
    "\n",
    "If the question is about Combinatorial Mathematics, please consider:\n",
    "Apply counting principles (permutations, combinations, inclusion-exclusion); use double counting or bijections; seek extremal values or invariants; consider recurrence relations or generating functions for an algebraic reformulation.\n",
    "\n",
    "If the question is about Algebraic Inequalities, please consider:\n",
    "Use classical inequalities (AM-GM, Cauchy-Schwarz, etc.); homogenize or normalize the expression; perform variable substitution or smoothing techniques; construct auxiliary functions to exploit monotonicity or convexity.\n",
    "\n",
    "If the question is about Geometric Constructions, please consider:\n",
    "Draw an accurate diagram and add necessary auxiliary lines or circles; analyze angles and proportions; employ geometric transformations (inversion, rotation, reflection) or use coordinate methods when appropriate.\n",
    "\n",
    "If the question is about Functional Equations, please consider:\n",
    "Substitute special values (like 0, 1, or equal variables) to simplify the equation; examine symmetry, periodicity, or monotonicity; prove uniqueness or construct all possible solutions through iterative substitution.\n",
    "\n",
    "If the question is about Others, please consider this way:\n",
    "First classify the problem and analyze its structure for hidden relationships or symmetry; combine methods from various fields as needed; stay flexible and adjust your approach based on specific features.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "though_prefix_chinese = \"\"\"<think>数学专家会对问题进行分类：识别为‘数论、组合数学、代数不等式、几何构造、函数方程、其它’类型中的一个类型。 \n",
    "\n",
    "\"\"\"\n",
    "problem_cate_china = \"\"\"\n",
    "如果题目是关于数论，请参考：\n",
    "理解题意并尝试构造简单例子；运用基本工具（素数分解、整除性质、模算术、欧几里得算法）；寻找不变量或递归结构；利用归纳法或转换简化条件。\n",
    "\n",
    "如果题目是关于组合数学，请参考：\n",
    "运用排列组合、容斥原理等计数技巧；尝试双重计数或构造双射；寻找极值条件或不变量；考虑递归关系或生成函数将问题转化为代数形式。\n",
    "\n",
    "如果题目是关于代数不等式，请参考：\n",
    "使用经典不等式工具（如 AM-GM、不等式、Cauchy-Schwarz 等）；对表达式进行归一化或同次化；采用变量替换或平滑化方法；构造辅助函数利用单调性或凸凹性证明不等式。\n",
    "\n",
    "如果题目是关于几何构造，请参考：\n",
    "仔细绘图并标记关键点，添加必要的辅助线或圆；分析角度和比例关系；使用几何变换（反演、旋转、平移）或坐标方法进行解析证明。\n",
    "\n",
    "如果题目是关于泛函方程，请参考：\n",
    "取特殊值（如 0、1 或令变量相等）简化方程；分析函数的对称性、周期性和单调性；证明解的唯一性或构造所有满足条件的解；利用迭代代入等技巧逐步缩小解的范围。\n",
    "\n",
    "如果题目属于其他类型，请参考：\n",
    "先判断题目所属领域，分析题目结构，寻找隐含的逻辑关系或对称性；必要时综合运用数论、组合、几何等多种方法；灵活调整解题策略，适时归纳总结或应用数学归纳法。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_starter_messages(question, index):\n",
    "    options = []\n",
    "    for _ in range(12):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\":thought_prefix_english +  \"You are a the most powerful math expert. Please solve the problems with deep resoning. You are careful and always recheck your conduction. You will never give answer directly until you have enough confidence. You should think step-by-step. Return final answer within \\\\boxed{}, after taking modulo 1000.\"},\n",
    "                {\"role\": \"user\", \"content\": question+problem_cate },\n",
    "            ]\n",
    "        )\n",
    "    for _ in range(1):    \n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": thought_prefix_english +  \"You are a helpful and harmless math expert. You should think step-by-step and you are good at reverse thinking to recheck your answer and fix all possible mistakes. After you get your final answer, take modulo 1000, and return the final answer within \\\\boxed{}.\"},\n",
    "                {\"role\": \"user\", \"content\": question+problem_cate},\n",
    "            ],\n",
    "        )\n",
    "    options.append(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": though_prefix_chinese + \"你是数学解题专家。请先仔细阅读题目，确保自己理解了题意和考点，再通过深度逐步推理来完整正确地解答问题，并把最终答案对1000取余数，放置于\\\\boxed{}中。\"},\n",
    "            {\"role\": \"user\", \"content\": question+problem_cate_china},\n",
    "        ],\n",
    "    )\n",
    "    return options[index%len(options)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "397988b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:49:07.956880Z",
     "iopub.status.busy": "2025-02-18T01:49:07.956608Z",
     "iopub.status.idle": "2025-02-18T01:49:07.962337Z",
     "shell.execute_reply": "2025-02-18T01:49:07.961753Z"
    },
    "papermill": {
     "duration": 0.018067,
     "end_time": "2025-02-18T01:49:07.963269",
     "exception": false,
     "start_time": "2025-02-18T01:49:07.945202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_for_question(question: str) -> int:\n",
    "    import os\n",
    "\n",
    "    selected_questions_only = True\n",
    "    # selected_questions_only = False\n",
    "    if selected_questions_only and not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        # if \"Triangle\" not in question:\n",
    "        #     return 210\n",
    "        if \"Triangle\" not in question and \"delightful\" not in question and \"George\" not in question:\n",
    "            return 210\n",
    "\n",
    "    if time.time() > cutoff_time:\n",
    "        return 210\n",
    "    \n",
    "    print(question)\n",
    "\n",
    "    num_seqs = MAX_NUM_SEQS\n",
    "    if time.time() > cutoff_times[-1]:\n",
    "        num_seqs = 2 * MAX_NUM_SEQS // 3\n",
    "    \n",
    "    list_of_messages = [create_starter_messages(question, index) for index in range(num_seqs)]\n",
    "\n",
    "    all_extracted_answers = []\n",
    "    for _ in range(1):\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        \n",
    "        if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                    \"question\": [question] * len(list_of_messages),\n",
    "                    \"message\": [messages[-1][\"content\"] for messages in list_of_messages],\n",
    "                }\n",
    "            )\n",
    "            df.to_csv(f\"{str(int(time.time() - start_time)).zfill(5)}.csv\", index=False)\n",
    "        \n",
    "        list_of_messages, extracted_answers = batch_message_filter(list_of_messages)\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "    \n",
    "    print(all_extracted_answers)\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    cutoff_times.pop()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cff1100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:49:07.985995Z",
     "iopub.status.busy": "2025-02-18T01:49:07.985761Z",
     "iopub.status.idle": "2025-02-18T01:49:07.989360Z",
     "shell.execute_reply": "2025-02-18T01:49:07.988780Z"
    },
    "papermill": {
     "duration": 0.016092,
     "end_time": "2025-02-18T01:49:07.990393",
     "exception": false,
     "start_time": "2025-02-18T01:49:07.974301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 999, inclusive.\n",
    "# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    \n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question)\n",
    "    print(question)\n",
    "    print(\"------\\n\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da84b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:49:08.013045Z",
     "iopub.status.busy": "2025-02-18T01:49:08.012824Z",
     "iopub.status.idle": "2025-02-18T01:49:08.015365Z",
     "shell.execute_reply": "2025-02-18T01:49:08.014788Z"
    },
    "papermill": {
     "duration": 0.014903,
     "end_time": "2025-02-18T01:49:08.016349",
     "exception": false,
     "start_time": "2025-02-18T01:49:08.001446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_for_question(\"Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1efcca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:49:08.039026Z",
     "iopub.status.busy": "2025-02-18T01:49:08.038805Z",
     "iopub.status.idle": "2025-02-18T01:49:08.066845Z",
     "shell.execute_reply": "2025-02-18T01:49:08.066199Z"
    },
    "papermill": {
     "duration": 0.040664,
     "end_time": "2025-02-18T01:49:08.068021",
     "exception": false,
     "start_time": "2025-02-18T01:49:08.027357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "043cf2d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:49:08.091172Z",
     "iopub.status.busy": "2025-02-18T01:49:08.090948Z",
     "iopub.status.idle": "2025-02-18T02:10:29.621788Z",
     "shell.execute_reply": "2025-02-18T02:10:29.621002Z"
    },
    "papermill": {
     "duration": 1281.543602,
     "end_time": "2025-02-18T02:10:29.622985",
     "exception": false,
     "start_time": "2025-02-18T01:49:08.079383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "1fce4b\n",
      "Find the three-digit number $n$ such that writing any other three-digit number $10^{2024}$ times in a row and $10^{2024}+2$ times in a row results in two numbers divisible by $n$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "480182\n",
      "Let $ABC$ be a triangle with $BC=108$, $CA=126$, and $AB=39$. Point $X$ lies on segment $AC$ such that $BX$ bisects $\\angle CBA$. Let $\\omega$ be the circumcircle of triangle $ABX$. Let $Y$ be a point on $\\omega$ different from $X$ such that $CX=CY$. Line $XY$ meets $BC$ at $E$. The length of the segment $BE$ can be written as $\\frac{m}{n}$, where $m$ and $n$ are coprime positive integers. Find $m+n$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "bbd91e\n",
      "Alice writes all positive integers from $1$ to $n$ on the board for some positive integer $n \\geq 11$. Bob then erases ten of them. The mean of the remaining numbers is $3000/37$. The sum of the numbers Bob erased is $S$. What is the remainder when $n \\times S$ is divided by $997$?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "71beb6\n",
      "For a positive integer $n$, let $S(n)$ denote the sum of the digits of $n$ in base 10. Compute $S(S(1)+S(2)+\\cdots+S(N))$ with $N=10^{100}-2$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "1acac0\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 14/14 [05:35<00:00, 23.93s/it, est. speed input: 20.16 toks/s, output: 251.75 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9387, 6659, 5017, 7232, 3932, 4327, 6415, 5908, 7632, 10207, 4441, 6682, 4475, 2042]\n",
      "[9387, 6659, 5017, 7232, 3932, 4327, 6415, 5908, 7632, 10207, 4441, 6682, 4475, 2042]\n",
      "[2042, 3932, 4327, 4441, 4475, 5017, 5908, 6415, 6659, 6682, 7232, 7632, 9387, 10207]\n",
      "['180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '160', '180', '180']\n",
      "180\n",
      "\n",
      "\n",
      "\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "349493\n",
      "We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 14/14 [07:59<00:00, 34.23s/it, est. speed input: 15.50 toks/s, output: 334.14 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 10655, 11767, 8360]\n",
      "[11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 10655, 11767, 8360]\n",
      "[8360, 10655, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 11760, 11767]\n",
      "['1', '1']\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "88c219\n",
      "For positive integers $x_1,\\ldots, x_n$ define $G(x_1, \\ldots, x_n)$ to be the sum of their $\\frac{n(n-1)}{2}$ pairwise greatest common divisors. We say that an integer $n \\geq 2$ is \\emph{artificial} if there exist $n$ different positive integers $a_1, ..., a_n$ such that \n",
      "\\[a_1 + \\cdots + a_n = G(a_1, \\ldots, a_n) +1.\\]\n",
      "Find the sum of all artificial integers $m$ in the range $2 \\leq m \\leq 40$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "057f8a\n",
      "Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "192e23\n",
      "Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 14/14 [07:46<00:00, 33.32s/it, est. speed input: 15.35 toks/s, output: 322.03 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11779, 7069, 10229, 11779, 11779, 11779, 11779, 11779, 9071, 11779, 7724, 11779, 11786, 10115]\n",
      "[11779, 7069, 10229, 11779, 11779, 11779, 11779, 11779, 9071, 11779, 7724, 11779, 11786, 10115]\n",
      "[7069, 7724, 9071, 10115, 10229, 11779, 11779, 11779, 11779, 11779, 11779, 11779, 11779, 11786]\n",
      "['750', '250', '0', '0']\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "a1d40b\n",
      "The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$. There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not. How many prime factors does $N$ have, counted with multiplicity?\n",
      "------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "#             '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv',\n",
    "            'reference.csv',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 9869096,
     "isSourceIdPinned": false,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "sourceId": 220483900,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 224053,
     "modelInstanceId": 202302,
     "sourceId": 236869,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 224053,
     "modelInstanceId": 206829,
     "sourceId": 242129,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1499.376815,
   "end_time": "2025-02-18T02:10:33.956476",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-18T01:45:34.579661",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "4abfc02422ea4724b79af2c1aece7f86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a393f418ab94ee3ae89c5874aab97ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d3217c99d8834c99b46375692789359a",
       "placeholder": "​",
       "style": "IPY_MODEL_c0d96f9c018748e8b4bf277a5ffe75e7",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "721db5cf5d3349a6b8a29233d249e441": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "904df476977e4cc5b809b5a266ddf93a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96d770b89cba43fda720e94670abd436": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_904df476977e4cc5b809b5a266ddf93a",
       "placeholder": "​",
       "style": "IPY_MODEL_eb213981b6c8444aaa4c98f62236e391",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:09&lt;00:00, 35.24s/it]\n"
      }
     },
     "9f35a254b8404d5daceca2b7cb3b9050": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5a393f418ab94ee3ae89c5874aab97ef",
        "IPY_MODEL_c86dcb817b8240878020d0126bde02ec",
        "IPY_MODEL_96d770b89cba43fda720e94670abd436"
       ],
       "layout": "IPY_MODEL_4abfc02422ea4724b79af2c1aece7f86",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c0d96f9c018748e8b4bf277a5ffe75e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c4f5e51d85fd42b5bcfa4ec07105553f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c86dcb817b8240878020d0126bde02ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_721db5cf5d3349a6b8a29233d249e441",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c4f5e51d85fd42b5bcfa4ec07105553f",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "d3217c99d8834c99b46375692789359a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb213981b6c8444aaa4c98f62236e391": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
